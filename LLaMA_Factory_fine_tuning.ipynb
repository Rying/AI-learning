{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "安装 LLaMA Factory 依赖"
      ],
      "metadata": {
        "id": "z6TBgZgOGh9q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iukwx22hFpi5",
        "outputId": "2902e5e6-02d3-40c3-8ff1-d8700766675f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 329, done.\u001b[K\n",
            "remote: Counting objects: 100% (329/329), done.\u001b[K\n",
            "remote: Compressing objects: 100% (264/264), done.\u001b[K\n",
            "remote: Total 329 (delta 80), reused 220 (delta 51), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (329/329), 8.99 MiB | 15.14 MiB/s, done.\n",
            "Resolving deltas: 100% (80/80), done.\n",
            "/content/LLaMA-Factory\n",
            "\u001b[0m\u001b[01;34massets\u001b[0m/       \u001b[01;34mdocker\u001b[0m/      LICENSE      pyproject.toml  requirements.txt  \u001b[01;34msrc\u001b[0m/\n",
            "CITATION.cff  \u001b[01;34mevaluation\u001b[0m/  Makefile     README.md       \u001b[01;34mscripts\u001b[0m/          \u001b[01;34mtests\u001b[0m/\n",
            "\u001b[01;34mdata\u001b[0m/         \u001b[01;34mexamples\u001b[0m/    MANIFEST.in  README_zh.md    setup.py\n",
            "Requirement already satisfied: torch==2.3.1 in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
            "Requirement already satisfied: torchvision==0.18.1 in /usr/local/lib/python3.10/dist-packages (0.18.1)\n",
            "Requirement already satisfied: torchaudio==2.3.1 in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (2024.9.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (2.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.18.1) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.18.1) (10.4.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.1) (12.6.85)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.1) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.1) (1.3.0)\n",
            "\u001b[33mWARNING: Skipping jax as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mObtaining file:///content/LLaMA-Factory\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers<=4.46.1,>=4.41.2 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (4.46.1)\n",
            "Requirement already satisfied: datasets<=3.1.0,>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (3.1.0)\n",
            "Requirement already satisfied: accelerate<=1.0.1,>=0.34.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (1.0.1)\n",
            "Requirement already satisfied: peft<=0.12.0,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (0.12.0)\n",
            "Requirement already satisfied: trl<=0.9.6,>=0.8.6 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (0.9.6)\n",
            "Requirement already satisfied: tokenizers<0.20.4,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (0.20.3)\n",
            "Requirement already satisfied: gradio<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (4.44.1)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (1.13.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (0.8.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (0.2.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (0.8.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (4.25.5)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (0.34.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (2.10.4)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (0.115.6)\n",
            "Requirement already satisfied: sse-starlette in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (2.2.1)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (3.10.0)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (0.7.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (6.0.2)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (1.26.4)\n",
            "Requirement already satisfied: av in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (14.0.1)\n",
            "Requirement already satisfied: tyro<0.9.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (0.8.14)\n",
            "Requirement already satisfied: liger-kernel in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (0.5.2)\n",
            "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (2.3.1)\n",
            "Requirement already satisfied: bitsandbytes>=0.39.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (0.45.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.0.1,>=0.34.0->llamafactory==0.9.2.dev0) (5.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.0.1,>=0.34.0->llamafactory==0.9.2.dev0) (0.27.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.0.1,>=0.34.0->llamafactory==0.9.2.dev0) (0.5.1)\n",
            "Requirement already satisfied: typing_extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes>=0.39.0->llamafactory==0.9.2.dev0) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (3.11.11)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (4.8.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (1.3.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (0.28.1)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (6.5.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (3.1.5)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (2.1.5)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (3.10.13)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (10.4.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (0.9.1)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (0.15.1)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (2.3.0)\n",
            "Requirement already satisfied: websockets<13.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (12.0)\n",
            "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->llamafactory==0.9.2.dev0) (0.41.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.9.2.dev0) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.9.2.dev0) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->llamafactory==0.9.2.dev0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic->llamafactory==0.9.2.dev0) (2.27.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (3.4.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.1->llamafactory==0.9.2.dev0) (12.6.85)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<=4.46.1,>=4.41.2->llamafactory==0.9.2.dev0) (2024.11.6)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro<0.9.0->llamafactory==0.9.2.dev0) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro<0.9.0->llamafactory==0.9.2.dev0) (13.9.4)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro<0.9.0->llamafactory==0.9.2.dev0) (1.7.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.9.2.dev0) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.9.2.dev0) (0.14.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->llamafactory==0.9.2.dev0) (2.5.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (1.2.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (1.3.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (1.18.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (1.0.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (3.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.2.dev0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.2.dev0) (2.18.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (1.5.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.1->llamafactory==0.9.2.dev0) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.2.dev0) (0.1.2)\n",
            "Building wheels for collected packages: llamafactory\n",
            "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llamafactory: filename=llamafactory-0.9.2.dev0-0.editable-py3-none-any.whl size=24662 sha256=8998c94f76cbe8db9573231557dc60a33a411c6b9774e33dd979ff48edf88281\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-6vl5zvy9/wheels/de/aa/c5/27b5682c5592b7c0eecc3e208f176dedf6b11a61cf2a910b85\n",
            "Successfully built llamafactory\n",
            "Installing collected packages: llamafactory\n",
            "  Attempting uninstall: llamafactory\n",
            "    Found existing installation: llamafactory 0.9.2.dev0\n",
            "    Uninstalling llamafactory-0.9.2.dev0:\n",
            "      Successfully uninstalled llamafactory-0.9.2.dev0\n",
            "Successfully installed llamafactory-0.9.2.dev0\n"
          ]
        }
      ],
      "source": [
        "%cd /content/\n",
        "%rm -rf LLaMA-Factory\n",
        "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
        "%cd LLaMA-Factory\n",
        "%ls\n",
        "!pip install torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1\n",
        "!pip uninstall -y jax\n",
        "!pip install -e .[torch,bitsandbytes,liger-kernel]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "61zdck8lF10w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "检查gpu"
      ],
      "metadata": {
        "id": "mKHbk75OGxcw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "try:\n",
        "  assert torch.cuda.is_available() is True\n",
        "except AssertionError:\n",
        "  print(\"需要 GPU 环境，申请教程：https://zhuanlan.zhihu.com/p/642542618\")"
      ],
      "metadata": {
        "id": "h3wxPYj-G0s7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "要训练的数据放在data目录下，准备训练数据ry.json"
      ],
      "metadata": {
        "id": "MxeiVfMq74NZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "sharegpt格式\n",
        "```\n",
        "[\n",
        "  {\n",
        "    \"conversations\": [\n",
        "      {\n",
        "        \"from\": \"human\",\n",
        "        \"value\": \"你能解释一下算法分析中的大O记法概念吗？\"\n",
        "      },\n",
        "      {\n",
        "        \"from\": \"gpt\",\n",
        "        \"value\": \"是的，大O表示法是一种描述算法性能随输入数据大小增加的方法。它用于估计算法在最坏情况下的时间复杂度或空间复杂度。大O中的“O”代表“数量级”，这意味着随着输入大小的增加，算法所需的操作数以一定的速率增长。\\n\\n例如，如果我们有一个需要n步才能解决大小为n的问题的算法，那么时间复杂度可以表示为O(n)，这意味着操作数随着输入大小线性增长。如果我们有一个需要n^2步才能解决大小为n的问题的算法，那么时间复杂度可以表示为O(n^2)，这意味着操作数随着输入大小二次方增长。\\n\\n还有其他几种大O表示法，如O(log n)、O(n log n)、O(2^n)等，它们代表了随着输入大小增加的不同操作数增长速率。通过分析算法的大O表示法，我们可以比较不同算法的性能，并选择给定问题最有效的算法。\\n\\n您是否有关于大O表示法或算法分析的特定问题？\"\n",
        "      }\n",
        "    ],\n",
        "    \"system\": \"\",\n",
        "    \"tools\": \"\"\n",
        "  }\n",
        "]\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "_JvsQYCb8IrX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "在dataset_info.json中添加训练数据ry.json"
      ],
      "metadata": {
        "id": "nE1jMzCMAOhK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "\"ry\": {\n",
        "    \"file_name\": \"ry.json\",\n",
        "    \"formatting\": \"sharegpt\",\n",
        "    \"columns\": {\n",
        "      \"messages\": \"conversations\",\n",
        "      \"system\": \"system\",\n",
        "      \"tools\": \"tools\"\n",
        "    },\n",
        "    \"tags\": {\n",
        "      \"role_tag\": \"from\",\n",
        "      \"content_tag\": \"value\",\n",
        "      \"user_tag\": \"human\",\n",
        "      \"assistant_tag\": \"gpt\"\n",
        "    }\n",
        "  }\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "1IPPnfT_AcPu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "基于qwen2.5:0.5b模型进行微调"
      ],
      "metadata": {
        "id": "dj3uWPav8c5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "args = dict(\n",
        "  stage=\"sft\",                        # 进行指令监督微调\n",
        "  do_train=True,\n",
        "  model_name_or_path=\"Qwen/Qwen2.5-0.5B\",\n",
        "  dataset=\"ry\",      #\n",
        "  template=\"qwen\",                     # 使用 llama3 提示词模板\n",
        "  finetuning_type=\"lora\",                   # 使用 LoRA 适配器来节省显存\n",
        "  lora_target=\"all\",                     # 添加 LoRA 适配器至全部线性层\n",
        "  output_dir=\"qwen_lora\",                  # 保存 LoRA 适配器的路径\n",
        "  per_device_train_batch_size=2,               # 批处理大小\n",
        "  gradient_accumulation_steps=4,               # 梯度累积步数\n",
        "  lr_scheduler_type=\"cosine\",                 # 使用余弦学习率退火算法\n",
        "  logging_steps=10,                      # 每 10 步输出一个记录\n",
        "  warmup_ratio=0.1,                      # 使用预热学习率\n",
        "  save_steps=1000,                      # 每 1000 步保存一个检查点\n",
        "  learning_rate=5e-5,                     # 学习率大小\n",
        "  num_train_epochs=3.0,                    # 训练轮数\n",
        "  max_samples=300,                      # 使用每个数据集中的 300 条样本\n",
        "  max_grad_norm=1.0,                     # 将梯度范数裁剪至 1.0\n",
        "  loraplus_lr_ratio=16.0,                   # 使用 LoRA+ 算法并设置 lambda=16.0\n",
        "  fp16=True,                         # 使用 float16 混合精度训练\n",
        "  use_liger_kernel=True,                   # 使用 Liger Kernel 加速训练\n",
        ")\n",
        "\n",
        "json.dump(args, open(\"train_qwen.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli train train_qwen.json"
      ],
      "metadata": {
        "id": "psywJyo75vt6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cb03328-1a36-4aac-edec-bfc0fa9985c5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n",
            "2025-01-14 07:21:08.980727: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-01-14 07:21:09.007417: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-01-14 07:21:09.018723: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-14 07:21:09.041546: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-01-14 07:21:10.684616: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[INFO|2025-01-14 07:21:16] llamafactory.hparams.parser:373 >> Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
            "[INFO|configuration_utils.py:679] 2025-01-14 07:21:16,410 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B/snapshots/060db6499f32faf8b98477b0a26969ef7d8b9987/config.json\n",
            "[INFO|configuration_utils.py:746] 2025-01-14 07:21:16,411 >> Model config Qwen2Config {\n",
            "  \"_name_or_path\": \"Qwen/Qwen2.5-0.5B\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151643,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 24,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_mrope\": false,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-14 07:21:16,502 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B/snapshots/060db6499f32faf8b98477b0a26969ef7d8b9987/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-14 07:21:16,502 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B/snapshots/060db6499f32faf8b98477b0a26969ef7d8b9987/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-14 07:21:16,502 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B/snapshots/060db6499f32faf8b98477b0a26969ef7d8b9987/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-14 07:21:16,502 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-14 07:21:16,502 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-14 07:21:16,502 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B/snapshots/060db6499f32faf8b98477b0a26969ef7d8b9987/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2475] 2025-01-14 07:21:16,835 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:679] 2025-01-14 07:21:18,141 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B/snapshots/060db6499f32faf8b98477b0a26969ef7d8b9987/config.json\n",
            "[INFO|configuration_utils.py:746] 2025-01-14 07:21:18,142 >> Model config Qwen2Config {\n",
            "  \"_name_or_path\": \"Qwen/Qwen2.5-0.5B\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151643,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 24,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_mrope\": false,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-14 07:21:18,247 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B/snapshots/060db6499f32faf8b98477b0a26969ef7d8b9987/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-14 07:21:18,248 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B/snapshots/060db6499f32faf8b98477b0a26969ef7d8b9987/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-14 07:21:18,248 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B/snapshots/060db6499f32faf8b98477b0a26969ef7d8b9987/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-14 07:21:18,248 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-14 07:21:18,248 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2025-01-14 07:21:18,248 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B/snapshots/060db6499f32faf8b98477b0a26969ef7d8b9987/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2475] 2025-01-14 07:21:18,572 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|2025-01-14 07:21:18] llamafactory.data.template:157 >> Add <|im_end|> to stop words.\n",
            "[INFO|2025-01-14 07:21:18] llamafactory.data.loader:157 >> Loading dataset ry.json...\n",
            "Generating train split: 1 examples [00:00, 82.71 examples/s]\n",
            "Converting format of dataset: 100% 1/1 [00:00<00:00, 191.57 examples/s]\n",
            "Running tokenizer on dataset: 100% 1/1 [00:00<00:00, 46.06 examples/s]\n",
            "training example:\n",
            "input_ids:\n",
            "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 107809, 104136, 100158, 107018, 101042, 101047, 26288, 46, 40814, 24339, 101290, 101037, 11319, 151645, 198, 151644, 77091, 198, 20412, 9370, 3837, 26288, 46, 51463, 24339, 101158, 53481, 107018, 102111, 99411, 31196, 20074, 92032, 100649, 104339, 1773, 99652, 100751, 100079, 100768, 24339, 18493, 31235, 100250, 99559, 101373, 20450, 102181, 26381, 57191, 101054, 102181, 26381, 1773, 26288, 46, 101047, 2073, 46, 854, 99661, 2073, 81800, 52334, 33590, 109883, 101067, 31196, 92032, 9370, 100649, 3837, 107018, 102974, 107631, 8863, 23031, 102495, 117171, 100023, 3407, 77557, 3837, 109198, 104133, 85106, 77, 64682, 101901, 100638, 92032, 17714, 77, 103936, 9370, 107018, 3837, 100624, 20450, 102181, 26381, 73670, 51463, 17714, 46, 1445, 69515, 109883, 40090, 8863, 101067, 31196, 92032, 43268, 33071, 100023, 1773, 109198, 104133, 85106, 77, 61, 17, 64682, 101901, 100638, 92032, 17714, 77, 103936, 9370, 107018, 3837, 100624, 20450, 102181, 26381, 73670, 51463, 17714, 46, 1445, 61, 17, 69515, 109883, 40090, 8863, 101067, 31196, 92032, 105935, 23384, 100023, 3407, 100626, 92894, 108464, 26288, 46, 51463, 24339, 3837, 29524, 46, 12531, 308, 8, 5373, 46, 1445, 1487, 308, 8, 5373, 46, 7, 17, 86167, 8, 49567, 3837, 104017, 99661, 34187, 101067, 31196, 92032, 100649, 106797, 40090, 8863, 100023, 117171, 1773, 67338, 101042, 107018, 104197, 46, 51463, 24339, 3837, 105773, 99792, 99604, 107018, 9370, 102111, 90395, 50404, 89012, 22382, 86119, 31235, 104775, 107018, 3407, 87026, 64471, 101063, 34204, 26288, 46, 51463, 24339, 57191, 107018, 101042, 9370, 105149, 86119, 11319, 151645, 198]\n",
            "inputs:\n",
            "<|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "你能解释一下算法分析中的大O记法概念吗？<|im_end|>\n",
            "<|im_start|>assistant\n",
            "是的，大O表示法是一种描述算法性能随输入数据大小增加的方法。它用于估计算法在最坏情况下的时间复杂度或空间复杂度。大O中的“O”代表“数量级”，这意味着随着输入大小的增加，算法所需的操作数以一定的速率增长。\n",
            "\n",
            "例如，如果我们有一个需要n步才能解决大小为n的问题的算法，那么时间复杂度可以表示为O(n)，这意味着操作数随着输入大小线性增长。如果我们有一个需要n^2步才能解决大小为n的问题的算法，那么时间复杂度可以表示为O(n^2)，这意味着操作数随着输入大小二次方增长。\n",
            "\n",
            "还有其他几种大O表示法，如O(log n)、O(n log n)、O(2^n)等，它们代表了随着输入大小增加的不同操作数增长速率。通过分析算法的大O表示法，我们可以比较不同算法的性能，并选择给定问题最有效的算法。\n",
            "\n",
            "您是否有关于大O表示法或算法分析的特定问题？<|im_end|>\n",
            "\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 20412, 9370, 3837, 26288, 46, 51463, 24339, 101158, 53481, 107018, 102111, 99411, 31196, 20074, 92032, 100649, 104339, 1773, 99652, 100751, 100079, 100768, 24339, 18493, 31235, 100250, 99559, 101373, 20450, 102181, 26381, 57191, 101054, 102181, 26381, 1773, 26288, 46, 101047, 2073, 46, 854, 99661, 2073, 81800, 52334, 33590, 109883, 101067, 31196, 92032, 9370, 100649, 3837, 107018, 102974, 107631, 8863, 23031, 102495, 117171, 100023, 3407, 77557, 3837, 109198, 104133, 85106, 77, 64682, 101901, 100638, 92032, 17714, 77, 103936, 9370, 107018, 3837, 100624, 20450, 102181, 26381, 73670, 51463, 17714, 46, 1445, 69515, 109883, 40090, 8863, 101067, 31196, 92032, 43268, 33071, 100023, 1773, 109198, 104133, 85106, 77, 61, 17, 64682, 101901, 100638, 92032, 17714, 77, 103936, 9370, 107018, 3837, 100624, 20450, 102181, 26381, 73670, 51463, 17714, 46, 1445, 61, 17, 69515, 109883, 40090, 8863, 101067, 31196, 92032, 105935, 23384, 100023, 3407, 100626, 92894, 108464, 26288, 46, 51463, 24339, 3837, 29524, 46, 12531, 308, 8, 5373, 46, 1445, 1487, 308, 8, 5373, 46, 7, 17, 86167, 8, 49567, 3837, 104017, 99661, 34187, 101067, 31196, 92032, 100649, 106797, 40090, 8863, 100023, 117171, 1773, 67338, 101042, 107018, 104197, 46, 51463, 24339, 3837, 105773, 99792, 99604, 107018, 9370, 102111, 90395, 50404, 89012, 22382, 86119, 31235, 104775, 107018, 3407, 87026, 64471, 101063, 34204, 26288, 46, 51463, 24339, 57191, 107018, 101042, 9370, 105149, 86119, 11319, 151645, 198]\n",
            "labels:\n",
            "是的，大O表示法是一种描述算法性能随输入数据大小增加的方法。它用于估计算法在最坏情况下的时间复杂度或空间复杂度。大O中的“O”代表“数量级”，这意味着随着输入大小的增加，算法所需的操作数以一定的速率增长。\n",
            "\n",
            "例如，如果我们有一个需要n步才能解决大小为n的问题的算法，那么时间复杂度可以表示为O(n)，这意味着操作数随着输入大小线性增长。如果我们有一个需要n^2步才能解决大小为n的问题的算法，那么时间复杂度可以表示为O(n^2)，这意味着操作数随着输入大小二次方增长。\n",
            "\n",
            "还有其他几种大O表示法，如O(log n)、O(n log n)、O(2^n)等，它们代表了随着输入大小增加的不同操作数增长速率。通过分析算法的大O表示法，我们可以比较不同算法的性能，并选择给定问题最有效的算法。\n",
            "\n",
            "您是否有关于大O表示法或算法分析的特定问题？<|im_end|>\n",
            "\n",
            "[INFO|configuration_utils.py:679] 2025-01-14 07:21:19,104 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B/snapshots/060db6499f32faf8b98477b0a26969ef7d8b9987/config.json\n",
            "[INFO|configuration_utils.py:746] 2025-01-14 07:21:19,106 >> Model config Qwen2Config {\n",
            "  \"_name_or_path\": \"Qwen/Qwen2.5-0.5B\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151643,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 24,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_mrope\": false,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "model.safetensors: 100% 988M/988M [00:23<00:00, 42.5MB/s]\n",
            "[INFO|modeling_utils.py:3937] 2025-01-14 07:21:42,664 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B/snapshots/060db6499f32faf8b98477b0a26969ef7d8b9987/model.safetensors\n",
            "[INFO|modeling_utils.py:1670] 2025-01-14 07:21:42,700 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:1096] 2025-01-14 07:21:42,708 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151643\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:4800] 2025-01-14 07:21:44,445 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4808] 2025-01-14 07:21:44,445 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2.5-0.5B.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
            "generation_config.json: 100% 138/138 [00:00<00:00, 839kB/s]\n",
            "[INFO|configuration_utils.py:1051] 2025-01-14 07:21:44,674 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B/snapshots/060db6499f32faf8b98477b0a26969ef7d8b9987/generation_config.json\n",
            "[INFO|configuration_utils.py:1096] 2025-01-14 07:21:44,674 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151643,\n",
            "  \"max_new_tokens\": 2048\n",
            "}\n",
            "\n",
            "[INFO|2025-01-14 07:21:44] llamafactory.model.model_utils.checkpointing:157 >> Gradient checkpointing enabled.\n",
            "[INFO|2025-01-14 07:21:44] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-01-14 07:21:44] llamafactory.model.adapter:157 >> Upcasting trainable params to float32.\n",
            "[INFO|2025-01-14 07:21:44] llamafactory.model.adapter:157 >> Fine-tuning method: LoRA\n",
            "[INFO|2025-01-14 07:21:44] llamafactory.model.model_utils.misc:157 >> Found linear modules: o_proj,k_proj,v_proj,down_proj,up_proj,gate_proj,q_proj\n",
            "[INFO|2025-01-14 07:21:45] llamafactory.model.loader:157 >> trainable params: 4,399,104 || all params: 498,431,872 || trainable%: 0.8826\n",
            "[WARNING|trainer.py:497] 2025-01-14 07:21:45,564 >> The model is not an instance of PreTrainedModel. No liger kernels will be applied.\n",
            "[INFO|trainer.py:698] 2025-01-14 07:21:45,695 >> Using auto half precision backend\n",
            "[INFO|2025-01-14 07:21:46] llamafactory.train.trainer_utils:157 >> Using LoRA+ optimizer with loraplus lr ratio 16.00.\n",
            "[INFO|trainer.py:2313] 2025-01-14 07:21:46,077 >> ***** Running training *****\n",
            "[INFO|trainer.py:2314] 2025-01-14 07:21:46,078 >>   Num examples = 1\n",
            "[INFO|trainer.py:2315] 2025-01-14 07:21:46,078 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2316] 2025-01-14 07:21:46,078 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:2319] 2025-01-14 07:21:46,078 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:2320] 2025-01-14 07:21:46,078 >>   Gradient Accumulation steps = 4\n",
            "[INFO|trainer.py:2321] 2025-01-14 07:21:46,078 >>   Total optimization steps = 3\n",
            "[INFO|trainer.py:2322] 2025-01-14 07:21:46,081 >>   Number of trainable parameters = 4,399,104\n",
            "[INFO|integration_utils.py:812] 2025-01-14 07:21:46,090 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/LLaMA-Factory/wandb/run-20250114_072205-8zpb4004\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mqwen_lora\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/renying0718-chinese-academy-of-sciences/llamafactory\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/renying0718-chinese-academy-of-sciences/llamafactory/runs/8zpb4004\u001b[0m\n",
            "100% 3/3 [00:01<00:00,  1.87it/s][INFO|trainer.py:3801] 2025-01-14 07:22:07,976 >> Saving model checkpoint to qwen_lora/checkpoint-3\n",
            "[INFO|configuration_utils.py:679] 2025-01-14 07:22:08,177 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B/snapshots/060db6499f32faf8b98477b0a26969ef7d8b9987/config.json\n",
            "[INFO|configuration_utils.py:746] 2025-01-14 07:22:08,178 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151643,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 24,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_mrope\": false,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2646] 2025-01-14 07:22:08,243 >> tokenizer config file saved in qwen_lora/checkpoint-3/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2655] 2025-01-14 07:22:08,243 >> Special tokens file saved in qwen_lora/checkpoint-3/special_tokens_map.json\n",
            "[INFO|trainer.py:2584] 2025-01-14 07:22:08,555 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 22.4738, 'train_samples_per_second': 0.133, 'train_steps_per_second': 0.133, 'train_loss': 1.6439565022786458, 'epoch': 3.0}\n",
            "100% 3/3 [00:02<00:00,  1.18it/s]\n",
            "[INFO|trainer.py:3801] 2025-01-14 07:22:08,560 >> Saving model checkpoint to qwen_lora\n",
            "[INFO|configuration_utils.py:679] 2025-01-14 07:22:08,779 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B/snapshots/060db6499f32faf8b98477b0a26969ef7d8b9987/config.json\n",
            "[INFO|configuration_utils.py:746] 2025-01-14 07:22:08,780 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151643,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 24,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_mrope\": false,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2646] 2025-01-14 07:22:08,835 >> tokenizer config file saved in qwen_lora/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2655] 2025-01-14 07:22:08,836 >> Special tokens file saved in qwen_lora/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  total_flos               =     1554GF\n",
            "  train_loss               =      1.644\n",
            "  train_runtime            = 0:00:22.47\n",
            "  train_samples_per_second =      0.133\n",
            "  train_steps_per_second   =      0.133\n",
            "[INFO|modelcard.py:449] 2025-01-14 07:22:09,030 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mqwen_lora\u001b[0m at: \u001b[34mhttps://wandb.ai/renying0718-chinese-academy-of-sciences/llamafactory/runs/8zpb4004\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250114_072205-8zpb4004/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "将微调模型转成gguf格式"
      ],
      "metadata": {
        "id": "P1d7GWmp8xh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "!git clone https://github.com/ggerganov/llama.cpp.git\n",
        "!python llama.cpp/convert_lora_to_gguf.py /content/LLaMA-Factory/qwen_lora\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pi9R9L8285CQ",
        "outputId": "55484d74-e429-490b-beed-aa44a4953a2d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 41240, done.\u001b[K\n",
            "remote: Counting objects: 100% (7557/7557), done.\u001b[K\n",
            "remote: Compressing objects: 100% (547/547), done.\u001b[K\n",
            "remote: Total 41240 (delta 7271), reused 7021 (delta 7009), pack-reused 33683 (from 2)\u001b[K\n",
            "Receiving objects: 100% (41240/41240), 72.68 MiB | 22.05 MiB/s, done.\n",
            "Resolving deltas: 100% (30059/30059), done.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/llama.cpp/convert_lora_to_gguf.py\", line 317, in <module>\n",
            "    lora_model = torch.load(input_model, map_location=\"cpu\", weights_only=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 1319, in load\n",
            "    with _open_file_like(f, \"rb\") as opened_file:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 659, in _open_file_like\n",
            "    return _open_file(name_or_buffer, mode)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 640, in __init__\n",
            "    super().__init__(open(name, mode))\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/LLaMA-Factory/qwen_lora/adapter_model.bin'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**创建Modelfile文件**\n",
        "\n",
        "\n",
        "```\n",
        "From qwen2.5:0.5b\n",
        "ADAPTER /content/LLaMA-Factory/qwen_lora/Qwen_Lora-F16-LoRA.gguf\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "93rh7XWa_86S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "安装ollama，本地运行微调之后的模型"
      ],
      "metadata": {
        "id": "PxwSdjLR-Z-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "!ollama pull qwen2.5:0.5b\n",
        "!ollama create qwen_lora -f ./Modelfile\n",
        "!ollama run qwen_lora"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asDKDw3F-gRs",
        "outputId": "883b022f-bd43-4dc5-bdd1-8c66301b5c49"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "############################################################################################# 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "Error: could not connect to ollama app, is it running?\n",
            "Error: could not connect to ollama app, is it running?\n",
            "Error: could not connect to ollama app, is it running?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "将微调模型压缩成zip包"
      ],
      "metadata": {
        "id": "6jm42jBR8kc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/LLaMA-Factory/file.zip /content/LLaMA-Factory/qwen_lora"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKrdxdRjyHcd",
        "outputId": "ba27f601-6364-4473-c863-96592ba18a96"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/LLaMA-Factory/qwen_lora/ (stored 0%)\n",
            "  adding: content/LLaMA-Factory/qwen_lora/checkpoint-3/ (stored 0%)\n",
            "  adding: content/LLaMA-Factory/qwen_lora/checkpoint-3/scheduler.pt (deflated 54%)\n",
            "  adding: content/LLaMA-Factory/qwen_lora/checkpoint-3/vocab.json (deflated 61%)\n",
            "  adding: content/LLaMA-Factory/qwen_lora/checkpoint-3/README.md (deflated 66%)\n",
            "  adding: content/LLaMA-Factory/qwen_lora/checkpoint-3/trainer_state.json (deflated 55%)\n",
            "  adding: content/LLaMA-Factory/qwen_lora/checkpoint-3/rng_state.pth (deflated 25%)\n",
            "  adding: content/LLaMA-Factory/qwen_lora/checkpoint-3/tokenizer_config.json (deflated 83%)\n",
            "  adding: content/LLaMA-Factory/qwen_lora/checkpoint-3/added_tokens.json (deflated 67%)\n",
            "  adding: content/LLaMA-Factory/qwen_lora/checkpoint-3/merges.txt (deflated 57%)\n",
            "  adding: content/LLaMA-Factory/qwen_lora/checkpoint-3/special_tokens_map.json (deflated 69%)\n",
            "  adding: content/LLaMA-Factory/qwen_lora/checkpoint-3/training_args.bin (deflated 51%)\n",
            "  adding: content/LLaMA-Factory/qwen_lora/checkpoint-3/adapter_model.safetensors (deflated 43%)\n",
            "  adding: content/LLaMA-Factory/qwen_lora/checkpoint-3/tokenizer.json (deflated 81%)\n",
            "  adding: content/LLaMA-Factory/qwen_lora/checkpoint-3/adapter_config.json (deflated 54%)\n",
            "  adding: content/LLaMA-Factory/qwen_lora/checkpoint-3/optimizer.pt (deflated 63%)\n",
            "  adding: content/LLaMA-Factory/qwen_lora/vocab.json (deflated 61%)\n",
            "  adding: content/LLaMA-Factory/qwen_lora/README.md (deflated 46%)\n",
            "  adding: content/LLaMA-Factory/qwen_lora/trainer_state.json (deflated 58%)\n",
            "  adding: content/LLaMA-Factory/qwen_lora/tokenizer_config.json (deflated 83%)\n",
            "  adding: content/LLaMA-Factory/qwen_lora/added_tokens.json (deflated 67%)\n",
            "  adding: content/LLaMA-Factory/qwen_lora/merges.txt (deflated 57%)\n",
            "  adding: content/LLaMA-Factory/qwen_lora/trainer_log.jsonl (deflated 28%)\n",
            "  adding: content/LLaMA-Factory/qwen_lora/special_tokens_map.json (deflated 69%)\n",
            "  adding: content/LLaMA-Factory/qwen_lora/training_args.bin (deflated 51%)\n",
            "  adding: content/LLaMA-Factory/qwen_lora/adapter_model.safetensors (deflated 43%)\n",
            "  adding: content/LLaMA-Factory/qwen_lora/all_results.json (deflated 40%)\n",
            "  adding: content/LLaMA-Factory/qwen_lora/tokenizer.json (deflated 81%)\n",
            "  adding: content/LLaMA-Factory/qwen_lora/runs/ (stored 0%)\n",
            "  adding: content/LLaMA-Factory/qwen_lora/runs/Jan14_07-21-16_8abaac96e171/ (stored 0%)\n",
            "  adding: content/LLaMA-Factory/qwen_lora/runs/Jan14_07-21-16_8abaac96e171/events.out.tfevents.1736839306.8abaac96e171.21003.0 (deflated 60%)\n",
            "  adding: content/LLaMA-Factory/qwen_lora/train_results.json (deflated 40%)\n",
            "  adding: content/LLaMA-Factory/qwen_lora/adapter_config.json (deflated 54%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "下载zip包"
      ],
      "metadata": {
        "id": "1FDvLozd8rJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/LLaMA-Factory/file.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "hMZWgNEPydo-",
        "outputId": "e1943beb-1232-4d68-c9e1-a540aba0f54e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_cc7d98fc-7b54-44eb-ba90-059ef6cbb12f\", \"file.zip\", 41441907)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}